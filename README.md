# ğŸ› RTO Big Data Web Scraper & AI-Powered Crawler

An **automated data extraction pipeline** that scrapes, cleans, and validates **Registered Training Organisation (RTO)** data from [training.gov.au](https://training.gov.au).  

This project combines **traditional scraping** (Selenium, BeautifulSoup, Playwright) with **LLM-powered crawling** for **JavaScript-heavy content**, enabling large-scale, up-to-date, and structured educational course datasets.

---

## âœ¨ Features

- ğŸ“Š **Processes 4,000+ RTOs** and **15,000+ courses** in one execution  
- ğŸ”„ **Automated pagination & deep crawling** from gov pages to official RTO sites  
- ğŸ¤– **LLM-assisted data extraction** with:
  - **Gemini 2.5 Flash** â€“ cost-efficient (<90k tokens/op)
  - **DeepSeek R1** â€“ semantic verification & keyword matching  
- ğŸ–¥ **Hybrid HTML/Markdown/JSON parsing** for dynamic content
- ğŸ—‚ **CSV & JSON output** for backend/API pipelines
- ğŸ§¹ **Data cleaning & normalization** with pandas
- âš¡ Headless browser mode for faster execution

---

## ğŸ›  Technologies Used

### Scraping & Automation

- **Python 3.11+**
- **Selenium** â€“ DOM scraping & interactions
- **BeautifulSoup4** â€“ HTML parsing
- **Playwright** â€“ JavaScript-rendered content scraping
- **Crawl4AI** â€“ AI-guided crawling & sub-URL targeting

### AI Models

- **Gemini 2.5 Flash** â€“ Structured extraction from complex layouts
- **DeepSeek-R1** â€“ Course existence & metadata verification

### Data Processing & Export

- **pandas** â€“ Data cleaning & transformation
- **CSV** â€“ Government schema-compatible export
- **JSON** â€“ API-ready format

---

## ğŸ“‚ Workflow

1. **Load Input CSV**  
   - Columns: `Code`, `Web Address`
   
2. **Phase 1 â€“ Government Scraping**  
   For each code, scrape:
   - `/summary` â€“ Organisation details
   - `/contacts` â€“ Contact info
   - `/addresses` â€“ Physical/postal addresses
   - `/qualifications` â€“ Offered qualifications  

3. **Phase 2 â€“ AI Verification**  
   - Visit each RTOâ€™s official website  
   - Search for each course using **LLM keyword prompts**  
   - Flag discrepancies & missing courses

4. **Phase 3 â€“ Cleaning & Structuring**  
   - Normalize dates, addresses, contact info  
   - Remove duplicates  
   - Match to CSV schema

5. **Output**  
   - Final **CSV**
   - Summary report of broken links & mismatches

---

## ğŸ”§ Getting Started

```bash
# 1. Clone the repository
git clone https://github.com/your-username/rto-big-data-scraper.git
cd rto-big-data-scraper

# 2. Create a virtual environment
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# 3. Install dependencies
pip install -r requirements.txt

# 4. Place your input CSV in /data/input.csv

# 5. Run the scraper
python scrape_rtos.py --input data/input.csv --output data/final_rtos.csv

